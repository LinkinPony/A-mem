[logging]
# Log level for the LLM interaction logger.
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level = DEBUG

# Whether to output logs to the console.
# Options: True, False
log_to_console = True

# Whether to output logs to a file.
# Options: True, False
log_to_file = True

# Path to the log file if log_to_file is True.
# Can be an absolute path or relative to the project root.
log_file_path = llm_interactions.log
[llm]
# Specifies the backend to use for Large Language Model interactions.
# Options: "gemini", "openai", "ollama"
backend = gemini

# The default model used for routine tasks like analysis and evolution.
# Should be a fast and cost-effective model.
# For Gemini: "gemini-1.5-flash-latest"
# For OpenAI: "gpt-3.5-turbo"
# For Ollama: model name like "llama3"
model_default = gemini-1.5-flash-latest

# The model used for complex, high-quality reasoning tasks like final answer synthesis.
# Should be a powerful and capable model.
# For Gemini: "gemini-1.5-pro-latest"
# For OpenAI: "gpt-4-turbo"
# For Ollama: model name like "llama3:70b"
model_large = gemini-2.5-flash-lite-preview-06-17

# --- API Keys (optional, can also be set as environment variables) ---
# If you don't set them here, the system will look for environment variables
# (e.g., GOOGLE_API_KEY, OPENAI_API_KEY).
# google_api_key = YOUR_GOOGLE_API_KEY_HERE
# openai_api_key = YOUR_OPENAI_API_KEY_HERE

# --- Ollama Specific Settings ---
# ollama_base_url = http://localhost:11434
